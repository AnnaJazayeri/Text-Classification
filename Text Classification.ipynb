{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6deb1bf",
   "metadata": {},
   "source": [
    "## Anna Jazayeri 131661209\n",
    "### Workshop 5\n",
    "### Text Mining BDM 550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64e40e",
   "metadata": {},
   "source": [
    "You may complete this workshop with a partner.<br>\n",
    "In this workshop, you are to create a classifier (naïve bayes) to classify the given dataset into 6 categories: AIMX, OWNX, CONT, BASE, NUMBER, and MISC.<br><br>\n",
    "Dataset:<br>\n",
    "Download the corpora: labeled_dataset.zip and unlabled_dataset.zip.<br>\n",
    "This corpus contains sentences from the abstract and introduction of scientific articles that have\n",
    "been annotated (labeled) using the following tags:<br><br>\n",
    "AIMX: \"A specific research goal of the current paper\"<br>\n",
    "OWNX: \"(Neutral) description of own work presented in current paper\"<br>\n",
    "CONT: \"Statements of comparison with or contrast to other work; weaknesses of other work\"<br>\n",
    "BASE: \"Statements of agreement with other work or continuation of other work\"<br>\n",
    "NUMBER: \"Contains a word related to numbers\"<br>\n",
    "MISC: \"(Neutral) description of other researchers' work\"<br><br>\n",
    "For example, the following three sentences from the labeled dataset are tagged with MISC,\n",
    "AIMX, and OWNX.<br>\n",
    "MISC The latter may also be applied to correct for an incidental large loop<br>\n",
    "AIMX In this paper we apply the idea to graphical models for continuous variables<br>\n",
    "OWNX We derive the loop corrected belief propagation equations for simple tractable<br>\n",
    "Gaussian models, yielding a message passing scheme that, besides the correct average\n",
    "marginals, also yields the correct variances<br><br><br>\n",
    "In addition to the labeled data, this corpus contains a set of unlabeled sentences. For example, the\n",
    "following two sentences are from the unlabeled dataset.<br>\n",
    "here are also more than 1,100 additional genome sequencing projects currently underway\n",
    "around the world CITATION, CITATION.<br>\n",
    "Convenient and effective computational methods are required to handle and analyze the\n",
    "immense amount of data generated by the whole-genome sequencing projects.<br><br>\n",
    "Your task<br>\n",
    "• Preprocess the text to remove any stop words or punctuations.<br>\n",
    "• Use TF-IDF to vectorize the sentences.<br>\n",
    "• Use Scikit learn to create a classifier. Then, use the classifier to label the sentences in the\n",
    "unlabeled dataset.<br>\n",
    "• Summarize your work and your findings in a few sentences.<br>\n",
    "• BONUS [Extra 5% added to any workshops]: Use other classification approaches to label\n",
    "the unlabeled sentences. Evaluate your work using precision, recall, and f1 score.<br><br>\n",
    "Submit<br>\n",
    "Submit your work (your code and the newly labeled dataset) on Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb07f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c43bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import zipfile\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b2ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86b6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383fa01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351bd9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectors for labeled sentences:\n",
      "  (0, 5)\t1.0\n",
      "  (1, 755)\t0.2386719935164609\n",
      "  (1, 206)\t0.33470012727386117\n",
      "  (1, 407)\t0.2847226640982237\n",
      "  (1, 426)\t0.32094783589825915\n",
      "  (1, 836)\t0.30971139568604245\n",
      "  (1, 539)\t0.2847226640982237\n",
      "  (1, 734)\t0.33470012727386117\n",
      "  (1, 280)\t0.33470012727386117\n",
      "  (1, 777)\t0.2217585069108034\n",
      "  (1, 419)\t0.2620598611961059\n",
      "  (1, 386)\t0.18417877959019113\n",
      "  (1, 35)\t0.2847226640982237\n",
      "  (1, 471)\t0.11427806653753046\n",
      "  (2, 245)\t0.18161065418452957\n",
      "  (2, 692)\t0.2024745415731555\n",
      "  (2, 629)\t0.2024745415731555\n",
      "  (2, 816)\t0.2024745415731555\n",
      "  (2, 834)\t0.2024745415731555\n",
      "  (2, 486)\t0.2024745415731555\n",
      "  (2, 338)\t0.2024745415731555\n",
      "  (2, 583)\t0.15853131187186775\n",
      "  (2, 760)\t0.2024745415731555\n",
      "  (2, 335)\t0.15853131187186775\n",
      "  (2, 541)\t0.1683129130823059\n",
      "  :\t:\n",
      "  (333, 130)\t0.407459802542017\n",
      "  (333, 548)\t0.24910510933814353\n",
      "  (333, 513)\t0.24910510933814353\n",
      "  (333, 186)\t0.20080691515376317\n",
      "  (333, 772)\t0.2136587165873279\n",
      "  (333, 156)\t0.25698811844964015\n",
      "  (333, 450)\t0.18007210460870182\n",
      "  (333, 828)\t0.19292390604226656\n",
      "  (333, 471)\t0.09482410304757878\n",
      "  (334, 510)\t0.44204071361242586\n",
      "  (334, 429)\t0.22102035680621293\n",
      "  (334, 737)\t0.41277338180971046\n",
      "  (334, 60)\t0.3920078794279544\n",
      "  (334, 712)\t0.16673660791126185\n",
      "  (334, 294)\t0.20638669090485523\n",
      "  (334, 88)\t0.1758068122423147\n",
      "  (334, 593)\t0.13791761803409466\n",
      "  (334, 595)\t0.16673660791126185\n",
      "  (334, 585)\t0.15949418962454986\n",
      "  (334, 496)\t0.1960039397139772\n",
      "  (334, 234)\t0.16673660791126185\n",
      "  (334, 513)\t0.3516136244846294\n",
      "  (334, 825)\t0.1758068122423147\n",
      "  (334, 156)\t0.18137027381261953\n",
      "  (334, 471)\t0.06692244621085709\n",
      "TF-IDF vectors for unlabeled sentences:\n",
      "  (0, 5)\t1.0\n",
      "  (1, 674)\t0.4848678363159081\n",
      "  (1, 73)\t0.6510603986574576\n",
      "  (1, 38)\t0.5839722070495499\n",
      "  (2, 749)\t0.45564920454706265\n",
      "  (2, 680)\t0.414725782854405\n",
      "  (2, 357)\t0.5481300401694382\n",
      "  (2, 122)\t0.45564920454706265\n",
      "  (2, 78)\t0.33514711529552066\n",
      "  (3, 622)\t0.8024660571969798\n",
      "  (3, 501)\t0.5966977685954035\n",
      "  (4, 789)\t0.3157194396617494\n",
      "  (4, 697)\t0.44093870248768363\n",
      "  (4, 687)\t0.47651536849552834\n",
      "  (4, 284)\t0.5017574147367336\n",
      "  (4, 228)\t0.47651536849552834\n",
      "  (5, 709)\t0.37488167170566206\n",
      "  (5, 697)\t0.39764533927426515\n",
      "  (5, 501)\t0.33646447585392675\n",
      "  (5, 366)\t0.48457618175912215\n",
      "  (5, 284)\t0.45249259430099514\n",
      "  (5, 165)\t0.38544772542521294\n",
      "  (6, 424)\t1.0\n",
      "  (7, 541)\t0.42852576927020886\n",
      "  (7, 203)\t0.49432036749839886\n",
      "  :\t:\n",
      "  (110, 31)\t0.5386475958130261\n",
      "  (110, 27)\t0.26878981368122196\n",
      "  (111, 684)\t0.30478550912155816\n",
      "  (111, 607)\t0.3209306545324724\n",
      "  (111, 501)\t0.47727527841437856\n",
      "  (111, 312)\t0.3209306545324724\n",
      "  (111, 186)\t0.2203744505065223\n",
      "  (111, 73)\t0.6095710182431163\n",
      "  (111, 33)\t0.23060660720956477\n",
      "  (112, 799)\t0.26514352966628235\n",
      "  (112, 646)\t0.4213796024788913\n",
      "  (112, 407)\t0.34042604807650134\n",
      "  (112, 212)\t0.22869079785250643\n",
      "  (112, 186)\t0.28935004194666786\n",
      "  (112, 118)\t0.6704615821870681\n",
      "  (112, 27)\t0.2251812250080176\n",
      "  (113, 638)\t0.2753719939654757\n",
      "  (113, 613)\t0.3832297579528616\n",
      "  (113, 293)\t0.32644741677236455\n",
      "  (113, 280)\t0.3639505146628992\n",
      "  (113, 271)\t0.34899637178667275\n",
      "  (113, 186)\t0.2631535695760848\n",
      "  (113, 158)\t0.3639505146628992\n",
      "  (113, 118)\t0.4065073049679538\n",
      "  (113, 78)\t0.2225328957739393\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import zipfile\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download and unzip the labeled and unlabeled datasets\n",
    "with zipfile.ZipFile('labeled_dataset.zip', 'r') as labeled_zip:\n",
    "    labeled_zip.extractall('labeled_dataset')\n",
    "\n",
    "with zipfile.ZipFile('unlabeled_dataset.zip', 'r') as unlabeled_zip:\n",
    "    unlabeled_zip.extractall('unlabeled_dataset')\n",
    "\n",
    "# Path to the labeled and unlabeled datasets\n",
    "labeled_data_path = 'labeled_dataset'\n",
    "unlabeled_data_path = 'unlabeled_dataset'\n",
    "\n",
    "# Preprocessing function to remove stopwords and punctuations\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and punctuations\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
    "    # Join the cleaned tokens into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Create a list to store preprocessed labeled and unlabeled sentences\n",
    "labeled_sentences = []\n",
    "unlabeled_sentences = []\n",
    "\n",
    "# Process labeled dataset\n",
    "for filename in os.listdir(labeled_data_path):\n",
    "    if os.path.isfile(os.path.join(labeled_data_path, filename)):\n",
    "        with open(os.path.join(labeled_data_path, filename), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                labeled_sentences.append(preprocess_text(line.strip()))\n",
    "\n",
    "# Process unlabeled dataset\n",
    "for filename in os.listdir(unlabeled_data_path):\n",
    "    if os.path.isfile(os.path.join(unlabeled_data_path, filename)):\n",
    "        with open(os.path.join(unlabeled_data_path, filename), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                unlabeled_sentences.append(preprocess_text(line.strip()))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the labeled sentences\n",
    "labeled_tfidf = tfidf_vectorizer.fit_transform(labeled_sentences)\n",
    "\n",
    "# Transform the unlabeled sentences using the same vectorizer\n",
    "unlabeled_tfidf = tfidf_vectorizer.transform(unlabeled_sentences)\n",
    "\n",
    "# Now you have TF-IDF vectors for both labeled and unlabeled sentences\n",
    "print(\"TF-IDF vectors for labeled sentences:\")\n",
    "print(labeled_tfidf)\n",
    "\n",
    "print(\"TF-IDF vectors for unlabeled sentences:\")\n",
    "print(unlabeled_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfeaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a147eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca07e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd6548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
