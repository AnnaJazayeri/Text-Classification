{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6deb1bf",
   "metadata": {},
   "source": [
    "## Anna Jazayeri 131661209\n",
    "### Workshop 5\n",
    "### Text Mining BDM 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18a3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a27c58",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24580539",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Task 1: Preprocess the text to remove any stop words or punctuations.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3bd225",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###: ### abstract ###\n",
      "\n",
      "MISC: Whole-genome transporter analyses have been conducted on 141 organisms whose complete genome sequences are available.\n",
      "\n",
      "OWNX: For each organism, the complete set of membrane transport systems was identified with predicted functions, and classified into protein families based on the transporter classification system.\n",
      "\n",
      "OWNX\twe: Organisms with larger genome sizes generally possessed a relatively greater number of transport systems.\n",
      "\n",
      "MISC: In prokaryotes and unicellular eukaryotes, the significant factor in the increase in transporter content with genome size was a greater diversity of transporter types.\n",
      "\n",
      "MISC: In contrast, in multicellular eukaryotes, greater number of paralogs in specific transporter families was the more important factor in the increase in transporter content with genome size.\n",
      "\n",
      "MISC: Both eukaryotic and prokaryotic intracellular pathogens and endosymbionts exhibited markedly limited transport capabilities.\n",
      "\n",
      "MISC: Hierarchical clustering of phylogenetic profiles of transporter families, derived from the presence or absence of a certain transporter family, showed that clustering patterns of organisms were correlated to both their evolutionary history and their overall physiology and lifestyles.\n",
      "\n",
      "###: ### introduction ###\n",
      "\n",
      "MISC: Membrane transport systems play essential roles in cellular metabolism and activities.\n",
      "\n",
      "MISC: Transporters function in the acquisition of organic nutrients, maintenance of ion homeostasis, extrusion of toxic and waste compounds, environmental sensing and cell communication, and other important cellular functions CITATION.\n",
      "\n",
      "MISC: Various transport systems differ in their putative membrane topology, energy coupling mechanisms, and substrate specificities CITATION.\n",
      "\n",
      "MISC: Among the prevailing energy sources are adenosine triphosphate, phosphoenolpyruvate, and chemiosmotic energy in the form of sodium ion or proton electrochemical gradients.\n",
      "\n",
      "MISC: The transporter classification system represents a systematic approach to classify transport systems according to their mode of transport, energy coupling mechanism, molecular phylogeny, and substrate specificity CITATION CITATION.\n",
      "\n",
      "OWNX: Transport mode and energy coupling mechanism serve as the primary basis for classification because of their relatively stable characteristics.\n",
      "\n",
      "MISC: There are four major classes of solute transporters in the transporter classification system: channels, primary transporters, secondary transporters, and group translocators.\n",
      "\n",
      "MISC: Transporters of unknown mechanism or function are included as a distinct class.\n",
      "\n",
      "MISC: Channels are energy-independent transporters that transport water, specific types of ions, or hydrophilic small molecules down a concentration or electrical gradient; they have higher rates of transport and lower stereospecificity than the other transporter classes.\n",
      "\n",
      "MISC: Primary active transporters couple the transport process to a primary source of energy.\n",
      "\n",
      "MISC: Secondary transporters utilize an ion or solute electrochemical gradient, e.g., proton/sodium motive force, to drive the transport process.\n",
      "\n",
      "MISC: E. coli LacY lactose permease CITATION, CITATION is probably one of the best characterized secondary transporters CITATION.\n",
      "\n",
      "MISC: Group translocators modify their substrates during the transport process.\n",
      "\n",
      "MISC: For example, E. coli MtlA mannitol PTS transporter phosphorylates exogenous mannitol using phosphoenolpyruvate as the phosphoryl donor and energy source and releases the phosphate ester, mannitol-1-P, into the cell cytoplasm CITATION, CITATION.\n",
      "\n",
      "MISC: Each transporter class is further classified into individual families and subfamilies according to their function, phylogeny, and/or substrate specificity CITATION .\n",
      "\n",
      "MISC: Since the advent of genomic sequencing technologies, the complete sequences of over 200 prokaryotic and eukaryotic genomes have been published to date, representing a wide range of species from archaea to human.\n",
      "\n",
      "MISC: There are also more than 1,100 additional genome sequencing projects currently underway around the world CITATION, CITATION.\n",
      "\n",
      "MISC: Convenient and effective computational methods are required to handle and analyze the immense amount of data generated by the whole-genome sequencing projects.\n",
      "\n",
      "MISC: An in-depth look at transport proteins is vital to the understanding of the metabolic capability of sequenced organisms.\n",
      "\n",
      "MISC: However, it is often problematic to annotate these transport proteins by current primary annotation methods because of the occurrence of large and complex transporter gene families, such as the ATP-binding cassette superfamily CITATION, CITATION and the major facilitator superfamily CITATION, CITATION, and the presence of multiple transporter gene paralogs in many organisms.\n",
      "\n",
      "MISC: We have been working on a systematic genome-wide analysis of cellular membrane transport systems.\n",
      "\n",
      "MISC: Previously, we reported a comprehensive analysis of the transport systems in 18 prokaryotic organisms CITATION, CITATION and in yeast CITATION.\n",
      "\n",
      "OWNX: Here we expand our analyses to 141 species and compare the fundamental differences in membrane transport systems in prokaryotes and eukaryotes.\n",
      "\n",
      "MISC: Phylogenetic profiling of transporter families and predicted substrates was utilized to investigate the relevance of transport capabilities to the overall physiology of prokaryotes and eukaryotes.\n",
      "\n",
      "###: ### abstract ###\n",
      "\n",
      "MISC: A central problem in the bioinformatics of gene regulation is to find the binding sites for regulatory proteins.\n",
      "\n",
      "MISC: One of the most promising approaches toward identifying these short and fuzzy sequence patterns is the comparative analysis of orthologous intergenic regions of related species.\n",
      "\n",
      "MISC: This analysis is complicated by various factors.\n",
      "\n",
      "MISC: First, one needs to take the phylogenetic relationship between the species into account in order to distinguish conservation that is due to the occurrence of functional sites from spurious conservation that is due to evolutionary proximity.\n",
      "\n",
      "MISC: Second, one has to deal with the complexities of multiple alignments of orthologous intergenic regions, and one has to consider the possibility that functional sites may occur outside of conserved segments.\n",
      "\n",
      "OWNX: Here we present a new motif sampling algorithm, PhyloGibbs, that runs on arbitrary collections of multiple local sequence alignments of orthologous sequences.\n",
      "\n",
      "MISC: The algorithm searches over all ways in which an arbitrary number of binding sites for an arbitrary number of transcription factors can be assigned to the multiple sequence alignments.\n",
      "\n",
      "MISC: These binding site configurations are scored by a Bayesian probabilistic model that treats aligned sequences by a model for the evolution of binding sites and background intergenic DNA.\n",
      "\n",
      "MISC: This model takes the phylogenetic relationship between the species in the alignment explicitly into account.\n",
      "\n",
      "OWNX: The algorithm uses simulated annealing and Monte Carlo Markov-chain sampling to rigorously assign posterior probabilities to all the binding sites that it reports.\n",
      "\n",
      "MISC: In tests on synthetic data and real data from five Saccharomyces species our algorithm performs significantly better than four other motif-finding algorithms, including algorithms that also take phylogeny into account.\n",
      "\n",
      "OWNX: Our results also show that, in contrast to the other algorithms, PhyloGibbs can make realistic estimates of the reliability of its predictions.\n",
      "\n",
      "OWNX\twe: Our tests suggest that, running on the five-species multiple alignment of a single gene's upstream region, PhyloGibbs on average recovers over 50 percent of all binding sites in S. cerevisiae at a specificity of about 50 percent, and 33 percent of all binding sites at a specificity of about 85 percent.\n",
      "\n",
      "MISC: We also tested PhyloGibbs on collections of multiple alignments of intergenic regions that were recently annotated, based on ChIP-on-chip data, to contain binding sites for the same TF.\n",
      "\n",
      "OWNX: We compared PhyloGibbs's results with the previous analysis of these data using six other motif-finding algorithms.\n",
      "\n",
      "MISC: For 16 of 21 TFs for which all other motif-finding methods failed to find a significant motif, PhyloGibbs did recover a motif that matches the literature consensus.\n",
      "\n",
      "MISC: In 11 cases where there was disagreement in the results we compiled lists of known target genes from the literature, and found that running PhyloGibbs on their regulatory regions yielded a binding motif matching the literature consensus in all but one of the cases.\n",
      "\n",
      "MISC: Interestingly, these literature gene lists had little overlap with the targets annotated based on the ChIP-on-chip data.\n",
      "\n",
      "MISC: The PhyloGibbs code can be downloaded from LINK or LINK.\n",
      "\n",
      "OWNX: The full set of predicted sites from our tests on yeast are available at LINK.\n",
      "\n",
      "###: ### introduction ###\n",
      "\n",
      "MISC: Transcription factors are proteins that bind in a sequence-specific manner to short DNA segments, most commonly in intergenic DNA upstream of a gene, to activate or suppress gene transcription.\n",
      "\n",
      "OWNX: Their DNA-binding domains recognize collections of short related DNA sequences.\n",
      "\n",
      "MISC: One generally finds that, although there is no unique combination of bases that is shared by all binding sites, and although different bases can occur at each position, there are clear biases in the distribution of bases that occur at each position of the binding sites.\n",
      "\n",
      "MISC: A common mathematical representation of a motif that takes this variability into account is a so-called weight matrix CITATION, CITATION w, whose components w i give the probabilities of finding base A, C, G, T at position i of a binding site.\n",
      "\n",
      "MISC: The main assumption underlying this mathematical representation is that the bases occurring at different positions of the binding site are probabilistically independent.\n",
      "\n",
      "MISC: This in turn follows, under some conditions CITATION, from the assumption that the binding energy of the protein to the DNA is a sum of pairwise contact energies between the individual nucleotides and the protein.\n",
      "\n",
      "MISC: There are several algorithms that are based on the WM representation that detect, ab initio, binding sites for a common TF in a collection of DNA sequences CITATION CITATION.\n",
      "\n",
      "OWNX: These algorithms broadly fall into two classes.\n",
      "\n",
      "OWNX: One class, of which MEME CITATION is the typical representative, searches the space of all WMs for the WM that can best explain the observed sequences.\n",
      "\n",
      "MISC: The class of Gibbs sampling algorithms, of which the Gibbs motif sampler CITATION, CITATION is the typical representative, instead samples the space of all multiple alignments of small sequence segments in search of the one that is most likely to consist of samples from a common WM.\n",
      "\n",
      "MISC: A crucial factor for the success of ab initio methods is the ratio of the number of binding sites to the total amount of DNA in the collection of sequences.\n",
      "\n",
      "MISC: That is, the larger the number of binding sites in the set, and the smaller the total amount of DNA, the more likely it is that ab initio methods can discover the binding sites among the other DNA sequences.\n",
      "\n",
      "MISC: In order to ensure a reasonable chance of success one thus needs to provide these methods with collections of sequences that are highly enriched with binding sites for a common TF.\n",
      "\n",
      "MISC: One possibility is to use sets of upstream regions from genes that appear co-regulated in microarray experiments or that were bound by a common TF in ChIP-on-chip experiments.\n",
      "\n",
      "OWNX: Another possibility is to use upstream regions of orthologous genes from related organisms.\n",
      "\n",
      "MISC: Here the assumption is that the regulation of the ancestor gene, and thus its binding sites, has been conserved in the orthologs that descend from it.\n",
      "\n",
      "MISC: This latter approach is in general complicated by a number of factors.\n",
      "\n",
      "MISC: When searching for regulatory sites in sequences that are not phylogenetically related, such as upstream regions of different genes from the same organism, one may simply look for short sequence motifs that are overrepresented among the input sequences.\n",
      "\n",
      "MISC: If the set of species from which the orthologous sequences derive are sufficiently diverged, one may simply choose to ignore the phylogenetic relationship between the sequences and treat the orthologous sequences in the same way as sequences that are not phylogenetically related.\n",
      "\n",
      "MISC: This was, for instance, the approach taken by McCue et al. CITATION, CITATION, where the Gibbs motif sampler algorithm CITATION, CITATION was used on upstream regions of proteo- bacteria.\n",
      "\n",
      "MISC: However, this approach is not applicable to datasets containing more closely related species, where some of the sequences will exhibit significant amounts of similarity simply because of their evolutionary proximity.\n",
      "\n",
      "MISC: Moreover, the amount of similarity will depend on the phylogenetic distance between the species, and it is clear that finding conserved sequence motifs between orthologous sequences from closely related species is much less indicative of function than finding sequence motifs that are conserved between distant species.\n",
      "\n",
      "MISC: One will in general thus have to distinguish conservation due to functional constraints from conservation due to evolutionary proximity, and to do this correctly, the phylogenetic relationship between the sequences has to be taken into account.\n",
      "\n",
      "MISC: A second challenge in using orthologous intergenic sequences from multiple species is the nontrivial structure of their multiple alignments.\n",
      "\n",
      "MISC: One typically finds a very heterogeneous pattern of conservation: well-conserved blocks of different sizes and covering different subsets of the species are interspersed with sequence segments that show little similarity with the sequences of the other species.\n",
      "\n",
      "MISC: The technique of phylogenetic footprinting, restricts attention to only those sequence segments in the genome of interest that show significant conservation with the other species.\n",
      "\n",
      "MISC: The conserved regions for multiple genes are then searched for common motifs by a variety of techniques.\n",
      "\n",
      "MISC: It is unclear, however, to what extent regulatory sites are restricted to such conserved segments.\n",
      "\n",
      "MISC: For instance, several studies of Drosophila and yeast CITATION CITATION have shown that there is no strong correlation between where experimentally annotated binding sites occur and whether that region is conserved.\n",
      "\n",
      "MISC: Thus, at least for yeast and flies, considerable information is lost by focusing on the conserved regions only.\n",
      "\n",
      "OWNX: We thus decided to retain the entire patchwork pattern of conserved sequence blocks and unaligned segments.\n",
      "\n",
      "MISC: Our strategy is implemented by a Gibbs sampling approach, and a preliminary account of the algorithm was presented in CITATION.\n",
      "\n",
      "MISC: The algorithm operates on arbitrary collections of both phylogenetically related sequences, such as orthologous intergenic regions, and sequences that are not phylogenetically related, such as upstream regions of different genes from the same organism.\n",
      "\n",
      "MISC: The phylogenetically related groups of sequences in the input are pre-aligned into local multiple alignments where clearly similar sequence segments are aligned into blocks and sequence segments of no or marginal similarity are left unaligned CITATION.\n",
      "\n",
      "OWNX: Although the algorithm can also take global multiple alignments as input, we believe that these often force phylogenetically unrelated segments into aligned blocks.\n",
      "\n",
      "MISC: This may adversely affect the performance of the algorithm.\n",
      "\n",
      "MISC: We score putative sites within blocks of aligned sequences with an evolutionary model that takes the phylogenetic relationships of the species into account, while putative sites in unaligned segments are treated as independent occurrences.\n",
      "\n",
      "OWNX: This Bayesian model defines a probability distribution over arbitrary placements of putative binding sites for multiple motifs, and we sample it with a Monte Carlo Markov chain.\n",
      "\n",
      "OWNX: We first use simulated annealing to search for the globally optimal configuration of binding sites.\n",
      "\n",
      "MISC: The motifs in this configuration are then tracked in a further sampling run to estimate realistic posterior probabilities for all the binding sites that the algorithm reports.\n",
      "\n",
      "MISC: Recently a number of other algorithms have been developed that search for regulatory motifs in groups of phylogenetically related sequences.\n",
      "\n",
      "MISC: Probably the first algorithm that was proposed is a generalization of the Consensus algorithm CITATION called PhyloCon CITATION.\n",
      "\n",
      "MISC: PhyloCon operates on sets of co-regulated genes and their orthologs.\n",
      "\n",
      "OWNX: It is a greedy algorithm that first finds ungapped alignments of similar sequence segments in sets of orthologous sequences, and then combines these alignments from different upstream regions into larger alignments.\n",
      "\n",
      "MISC: This algorithm does not take any phylogenetic information into account, i.e., closely related sequences are treated the same as distantly related sequences.\n",
      "\n",
      "MISC: Other drawbacks of this algorithm are that it assumes that each motif will have exactly one site in each of the intergenic regions and that it assumes that this site is conserved in all orthologs.\n",
      "\n",
      "MISC: More closely related to PhyloGibbs's approach are two recent algorithms CITATION, CITATION that generalize MEME CITATION to take the phylogenetic relationships between species into account.\n",
      "\n",
      "MISC: The main difference between EMnEM and PhyME is that PhyME uses the same evolutionary model for the evolution of binding sites as PhyloGibbs, which takes into account that binding sites evolve under constraints set by a WM, whereas EMnEM simply assumes an overall slower rate of evolution in binding sites than in background sequences.\n",
      "\n",
      "MISC: Another difference is that PhyME, like PhyloGibbs, treats the multiple alignment more flexibly than EMnEM, which demands a global multiple alignment.\n",
      "\n",
      "OWNX: The main difference between PhyloGibbs and these algorithms is of course that PhyloGibbs takes a motif sampling approach, which allows us to search for multiple motifs in parallel, whereas PhyME and EMnEM use expectation maximization to search for one WM at a time.\n",
      "\n",
      "OWNX: In the following sections, we first describe our Bayesian model that assigns a posterior probability to each configuration of binding sites for multiple motifs assigned to the input sequences.\n",
      "\n",
      "MISC: We start by describing the model for phylogenetically unrelated sequences, which is essentially equivalent to the model used in the Gibbs motif sampler CITATION, CITATION, and then describe how this model is extended to datasets that contain phylogenetically related sequences.\n",
      "\n",
      "OWNX: After that we describe the move set with which we search the state space of all possible configurations, and the annealing and tracking strategy that we use to identify the significant groups of sites.\n",
      "\n",
      "OWNX: We then present examples of the performance of ours and other algorithms on both synthetic and real data.\n",
      "\n",
      "OWNX: The synthetic datasets consist of mixtures of WM samples and random sequences, which is in accordance with assumptions that all algorithms make.\n",
      "\n",
      "MISC: This allows us to compare the performance of the algorithms in an idealized situation that does not contain the complexities of real data.\n",
      "\n",
      "MISC: These tests also show to what extent binding sites can be recovered for this idealized data as a function of the quality of the WMs, the number of sites available, and the number of species available and their phylogenetic distances.\n",
      "\n",
      "MISC: For our tests on real data we use 200 upstream regions from Saccharomyces cerevisiae that have known binding sites from the collection CITATION, and compare the ability of the different algorithms to recover these sites when running on multiple alignments of the orthologs of these upstream regions from recently sequenced Saccharomyces genomes CITATION, CITATION.\n",
      "\n",
      "OWNX: Finally, we run PhyloGibbs on collections of upstream region alignments that were annotated in CITATION to contain binding sites for a common TF based on data from ChIP-on-chip experiments, and we extensively compare PhyloGibbs' results with the annotations in CITATION and with the literature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download and unzip the labeled and unlabeled datasets\n",
    "with zipfile.ZipFile('labeled_dataset.zip', 'r') as labeled_zip:\n",
    "    labeled_zip.extractall('labeled_dataset')\n",
    "\n",
    "with zipfile.ZipFile('unlabeled_dataset.zip', 'r') as unlabeled_zip:\n",
    "    unlabeled_zip.extractall('unlabeled_dataset')\n",
    "\n",
    "# Step 1: Load the labeled dataset\n",
    "labeled_dataset_path = 'labeled_dataset'\n",
    "unlabeled_dataset_path = 'unlabeled_dataset'\n",
    "\n",
    "labeled_data = []\n",
    "labels = []\n",
    "\n",
    "for file in os.listdir(labeled_dataset_path):\n",
    "    if os.path.isfile(os.path.join(labeled_dataset_path, file)):\n",
    "        with open(os.path.join(labeled_dataset_path, file), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    label, sentence = parts\n",
    "                    labeled_data.append(sentence)\n",
    "                    labels.append(label)\n",
    "\n",
    "# Feature extraction using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(labeled_data)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_vectorized, labels)\n",
    "\n",
    "# Use the trained classifier to predict labels for unlabeled data\n",
    "unlabeled_data = []\n",
    "\n",
    "for file in os.listdir(unlabeled_dataset_path):\n",
    "    if os.path.isfile(os.path.join(unlabeled_dataset_path, file)):\n",
    "        with open(os.path.join(unlabeled_dataset_path, file), 'r', encoding='utf-8') as f:\n",
    "            unlabeled_data.extend(f.readlines())\n",
    "\n",
    "# Vectorize the unlabeled data\n",
    "unlabeled_data_vectorized = vectorizer.transform(unlabeled_data)\n",
    "\n",
    "# Predict labels for the unlabeled data\n",
    "predicted_labels = classifier.predict(unlabeled_data_vectorized)\n",
    "\n",
    "# Print or save the predicted labels for further use\n",
    "for sentence, label in zip(unlabeled_data, predicted_labels):\n",
    "    print(f\"{label}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81a697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8352b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a453e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb42458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08b9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe106705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26333bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   annotate1       1.00      1.00      1.00         1\n",
      "   annotate4       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "1.txt: annotate4\n",
      "2.txt: annotate4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download and unzip the labeled and unlabeled datasets\n",
    "with zipfile.ZipFile('labeled_dataset.zip', 'r') as labeled_zip:\n",
    "    labeled_zip.extractall('labeled_dataset')\n",
    "\n",
    "with zipfile.ZipFile('unlabeled_dataset.zip', 'r') as unlabeled_zip:\n",
    "    unlabeled_zip.extractall('unlabeled_dataset')\n",
    "\n",
    "# Step 1: Load the labeled dataset\n",
    "labeled_dataset_path = 'labeled_dataset'\n",
    "unlabeled_dataset_path = 'unlabeled_dataset'\n",
    "\n",
    "labeled_data = []\n",
    "labels = []\n",
    "\n",
    "for file in os.listdir(labeled_dataset_path):\n",
    "    if os.path.isfile(os.path.join(labeled_dataset_path, file)):\n",
    "        with open(os.path.join(labeled_dataset_path, file), 'r', encoding='utf-8') as f:\n",
    "            labeled_data.append(f.read())\n",
    "            labels.append(file.split('_')[1].split('.')[0])\n",
    "\n",
    "# Step 2: Split the labeled dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(labeled_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Feature extraction using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Step 5: Evaluate the classifier on the test set\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Use the trained classifier to predict labels for unlabeled data\n",
    "unlabeled_data = []\n",
    "\n",
    "for file in os.listdir(unlabeled_dataset_path):\n",
    "    if os.path.isfile(os.path.join(unlabeled_dataset_path, file)):\n",
    "        with open(os.path.join(unlabeled_dataset_path, file), 'r', encoding='utf-8') as f:\n",
    "            unlabeled_data.append(f.read())\n",
    "\n",
    "# Vectorize the unlabeled data\n",
    "unlabeled_data_vectorized = vectorizer.transform(unlabeled_data)\n",
    "\n",
    "# Predict labels for the unlabeled data\n",
    "predicted_labels = classifier.predict(unlabeled_data_vectorized)\n",
    "\n",
    "# Print or save the predicted labels for further use\n",
    "for file, label in zip(os.listdir(unlabeled_dataset_path), predicted_labels):\n",
    "    print(f\"{file}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24ed91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc539c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c8a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cf502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db47363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1bd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fc541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60fcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcee6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb86b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all I must loading the files \n",
    "# import zipfile\n",
    "# Download and unzip the labeled and unlabeled datasets\n",
    "with zipfile.ZipFile('labeled_dataset.zip', 'r') as labeled_zip:\n",
    "    labeled_zip.extractall('labeled_dataset')\n",
    "\n",
    "with zipfile.ZipFile('unlabeled_dataset.zip', 'r') as unlabeled_zip:\n",
    "    unlabeled_zip.extractall('unlabeled_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63934d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the labeled and unlabeled datasets\n",
    "label_path = 'labeled_dataset'\n",
    "unlabel_path = 'unlabeled_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6c8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels and lines\n",
    "labels = []\n",
    "lines = []\n",
    "\n",
    "for line in label_path:\n",
    "    # Split the line into words\n",
    "    words = line.split()\n",
    "    \n",
    "    # Extract the label (the first word) and the rest of the line\n",
    "    label = words[0]\n",
    "    content = ' '.join(words[1:])\n",
    "    \n",
    "    # Append to the lists\n",
    "    labels.append(label)\n",
    "    lines.append(content)\n",
    "\n",
    "# Write the labels to a new file\n",
    "with open('all_labels.txt', 'w') as label_file:\n",
    "    label_file.write('\\n'.join(labels))\n",
    "\n",
    "# Write the lines to a new file\n",
    "with open('label_lines.txt', 'w') as lines_file:\n",
    "    lines_file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287634e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753bd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3c2fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unlabel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(unlabel_data):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(unlabel_data, file)):\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43munlabel\u001b[49m, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     53\u001b[0m                 clean_unlabel\u001b[38;5;241m.\u001b[39mappend(remove_stop_punc(line\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unlabel' is not defined"
     ]
    }
   ],
   "source": [
    "# Now I must remove  the stopwords and punctuations\n",
    "# I create a funcion for this \n",
    "# from nltk.corpus import stopwords\n",
    "# import os\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "\n",
    "# Function to extract labels and texts from a file\n",
    "def extract_labels_and_texts(file_path):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Splitting each line into label and text\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                label, text = parts\n",
    "                labels.append(label)\n",
    "                label_data_texts.append(text)\n",
    "    return labels, label_data_texts\n",
    "\n",
    "# Lists to store labels and texts\n",
    "labels = []\n",
    "label_data_texts = []\n",
    "\n",
    "def remove_stop_punc(txt):\n",
    "    # Tokenizing the text\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    # removing the stopwords and punctuations\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
    "    # now the cleaned tokens must be join together as a string\n",
    "    clean_txt = ' '.join(tokens)\n",
    "    return clean_txt\n",
    "\n",
    "# now we should use a for loop to cleaning each corpura and then \n",
    "# and then storing those cleaned corpura inside of a list \n",
    "clean_label = []\n",
    "clean_unlabel = []\n",
    "\n",
    "# a for loop for cleaning the labeled texts\n",
    "for file in os.listdir(label_data):\n",
    "    if os.path.isfile(os.path.join(label_data, file)):\n",
    "        with open(os.path.join(label_data, file), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                clean_label.append(remove_stop_punc(line.strip()))\n",
    "\n",
    "                \n",
    "# a for loop for cleaning the unlabeled texts\n",
    "for file in os.listdir(unlabel_data):\n",
    "    if os.path.isfile(os.path.join(unlabel_data, file)):\n",
    "        with open(os.path.join(unlabel, file), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                clean_unlabel.append(remove_stop_punc(line.strip()))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b64674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'misc although internet level topology extensively studied past years little known details taxonomy',\n",
       " 'misc node represent wide variety organizations e g large isp small private business university vastly different network characteristics external connectivity patterns network growth tendencies properties hardly neglect working veracious internet representations simulation environments',\n",
       " 'aimx paper introduce radically new approach based machine learning techniques map ases internet natural taxonomy',\n",
       " 'ownx successfully classify number number percent ases expected accuracy number number percent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see our clean label list just for first 5 lines\n",
    "clean_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa467ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'transporter analyses conducted 141 organisms whose complete genome sequences available',\n",
       " 'organism complete set membrane transport systems identified predicted functions classified protein families based transporter classification system',\n",
       " 'organisms larger genome sizes generally possessed relatively greater number transport systems',\n",
       " 'prokaryotes unicellular eukaryotes significant factor increase transporter content genome size greater diversity transporter types']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see our clean unlabel list just for first 5 lines\n",
    "clean_unlabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a20cc",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Task 2: Use TF-IDF to vectorize the sentences.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ba30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizing with TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fiting and transforming our clean label data\n",
    "tfidf_label = tfidf_vectorizer.fit_transform(clean_label)\n",
    "# Fiting and transforming our clean unlabel data\n",
    "tfidf_unlabel = tfidf_vectorizer.transform(clean_unlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19599ee4",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Task 3: Use Scikit learn to create a (naïve bayes) classifier.</span>\n",
    "\n",
    "<span style=\"color:blue;\">**First create a classifier (naïve bayes) to classify the given dataset into 6 categories: (AIMX, OWNX, CONT, BASE, NUMBER, and MISC).<br>Then, use the classifier to label the sentences in the unlabeled dataset.**</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a7fd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 840 features, but MultinomialNB is expecting 836 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Predict labels for the unlabeled data\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_unlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Print the classification report\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, nb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test), target_names\u001b[38;5;241m=\u001b[39mcategories))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:101\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:574\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 840 features, but MultinomialNB is expecting 836 features as input."
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Split the labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_label, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the unlabeled data\n",
    "predicted_labels = nb_classifier.predict(tfidf_unlabel)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, nb_classifier.predict(X_test), target_names=categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7891a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3990e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ff540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f596c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895754b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c08575b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Split the labeled data into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(tfidf_label, \u001b[43mlabels\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m     16\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Labels for the categories\n",
    "categories = ['AIMX', 'OWNX', 'CONT', 'BASE', 'NUMBER', 'MISC']\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Split the labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_label, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the unlabeled data\n",
    "predicted_labels = nb_classifier.predict(tfidf_unlabel)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, nb_classifier.predict(X_test), target_names=categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfa851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9331fb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93471034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbc7511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05970149253731343\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      1.00      0.03         1\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          26       0.00      0.00      0.00         1\n",
      "          43       0.00      0.00      0.00         1\n",
      "          47       0.00      0.00      0.00         1\n",
      "          50       0.00      0.00      0.00         1\n",
      "          62       0.00      0.00      0.00         0\n",
      "          63       0.00      0.00      0.00         1\n",
      "          65       0.00      0.00      0.00         1\n",
      "          68       0.00      0.00      0.00         0\n",
      "          71       0.00      0.00      0.00         0\n",
      "          73       0.00      0.00      0.00         1\n",
      "          74       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         1\n",
      "          90       0.00      0.00      0.00         0\n",
      "          93       0.00      0.00      0.00         1\n",
      "          97       0.00      0.00      0.00         0\n",
      "         105       0.00      0.00      0.00         1\n",
      "         107       0.00      0.00      0.00         1\n",
      "         109       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         1\n",
      "         119       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         1\n",
      "         125       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00         1\n",
      "         138       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.00      0.00      0.00         1\n",
      "         151       0.00      0.00      0.00         1\n",
      "         157       1.00      1.00      1.00         1\n",
      "         163       0.00      0.00      0.00         1\n",
      "         167       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         175       0.00      0.00      0.00         1\n",
      "         177       0.00      0.00      0.00         1\n",
      "         179       0.00      0.00      0.00         1\n",
      "         182       0.00      0.00      0.00         1\n",
      "         189       0.00      0.00      0.00         1\n",
      "         192       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         202       0.00      0.00      0.00         1\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         208       0.00      0.00      0.00         0\n",
      "         211       0.00      0.00      0.00         1\n",
      "         213       0.00      0.00      0.00         1\n",
      "         214       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         1\n",
      "         217       0.00      0.00      0.00         1\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         247       0.00      0.00      0.00         1\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         1\n",
      "         251       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.06        67\n",
      "   macro avg       0.03      0.04      0.03        67\n",
      "weighted avg       0.05      0.06      0.05        67\n",
      "\n",
      "Prediction for sentence 1: AIMX\n",
      "Prediction for sentence 2: AIMX\n",
      "Prediction for sentence 3: AIMX\n",
      "Prediction for sentence 4: AIMX\n",
      "Prediction for sentence 5: AIMX\n",
      "Prediction for sentence 6: AIMX\n",
      "Prediction for sentence 7: AIMX\n",
      "Prediction for sentence 8: AIMX\n",
      "Prediction for sentence 9: OWNX\n",
      "Prediction for sentence 10: AIMX\n",
      "Prediction for sentence 11: AIMX\n",
      "Prediction for sentence 12: AIMX\n",
      "Prediction for sentence 13: AIMX\n",
      "Prediction for sentence 14: AIMX\n",
      "Prediction for sentence 15: AIMX\n",
      "Prediction for sentence 16: AIMX\n",
      "Prediction for sentence 17: AIMX\n",
      "Prediction for sentence 18: AIMX\n",
      "Prediction for sentence 19: AIMX\n",
      "Prediction for sentence 20: AIMX\n",
      "Prediction for sentence 21: AIMX\n",
      "Prediction for sentence 22: AIMX\n",
      "Prediction for sentence 23: AIMX\n",
      "Prediction for sentence 24: AIMX\n",
      "Prediction for sentence 25: AIMX\n",
      "Prediction for sentence 26: AIMX\n",
      "Prediction for sentence 27: AIMX\n",
      "Prediction for sentence 28: AIMX\n",
      "Prediction for sentence 29: AIMX\n",
      "Prediction for sentence 30: AIMX\n",
      "Prediction for sentence 31: AIMX\n",
      "Prediction for sentence 32: AIMX\n",
      "Prediction for sentence 33: AIMX\n",
      "Prediction for sentence 34: AIMX\n",
      "Prediction for sentence 35: AIMX\n",
      "Prediction for sentence 36: AIMX\n",
      "Prediction for sentence 37: AIMX\n",
      "Prediction for sentence 38: AIMX\n",
      "Prediction for sentence 39: AIMX\n",
      "Prediction for sentence 40: AIMX\n",
      "Prediction for sentence 41: AIMX\n",
      "Prediction for sentence 42: AIMX\n",
      "Prediction for sentence 43: AIMX\n",
      "Prediction for sentence 44: AIMX\n",
      "Prediction for sentence 45: AIMX\n",
      "Prediction for sentence 46: AIMX\n",
      "Prediction for sentence 47: AIMX\n",
      "Prediction for sentence 48: AIMX\n",
      "Prediction for sentence 49: AIMX\n",
      "Prediction for sentence 50: AIMX\n",
      "Prediction for sentence 51: AIMX\n",
      "Prediction for sentence 52: AIMX\n",
      "Prediction for sentence 53: AIMX\n",
      "Prediction for sentence 54: AIMX\n",
      "Prediction for sentence 55: OWNX\n",
      "Prediction for sentence 56: AIMX\n",
      "Prediction for sentence 57: AIMX\n",
      "Prediction for sentence 58: AIMX\n",
      "Prediction for sentence 59: AIMX\n",
      "Prediction for sentence 60: AIMX\n",
      "Prediction for sentence 61: AIMX\n",
      "Prediction for sentence 62: AIMX\n",
      "Prediction for sentence 63: AIMX\n",
      "Prediction for sentence 64: AIMX\n",
      "Prediction for sentence 65: AIMX\n",
      "Prediction for sentence 66: AIMX\n",
      "Prediction for sentence 67: AIMX\n",
      "Prediction for sentence 68: AIMX\n",
      "Prediction for sentence 69: AIMX\n",
      "Prediction for sentence 70: AIMX\n",
      "Prediction for sentence 71: AIMX\n",
      "Prediction for sentence 72: AIMX\n",
      "Prediction for sentence 73: AIMX\n",
      "Prediction for sentence 74: AIMX\n",
      "Prediction for sentence 75: AIMX\n",
      "Prediction for sentence 76: AIMX\n",
      "Prediction for sentence 77: AIMX\n",
      "Prediction for sentence 78: AIMX\n",
      "Prediction for sentence 79: AIMX\n",
      "Prediction for sentence 80: AIMX\n",
      "Prediction for sentence 81: AIMX\n",
      "Prediction for sentence 82: AIMX\n",
      "Prediction for sentence 83: AIMX\n",
      "Prediction for sentence 84: AIMX\n",
      "Prediction for sentence 85: AIMX\n",
      "Prediction for sentence 86: AIMX\n",
      "Prediction for sentence 87: AIMX\n",
      "Prediction for sentence 88: AIMX\n",
      "Prediction for sentence 89: AIMX\n",
      "Prediction for sentence 90: AIMX\n",
      "Prediction for sentence 91: AIMX\n",
      "Prediction for sentence 92: AIMX\n",
      "Prediction for sentence 93: AIMX\n",
      "Prediction for sentence 94: AIMX\n",
      "Prediction for sentence 95: AIMX\n",
      "Prediction for sentence 96: AIMX\n",
      "Prediction for sentence 97: AIMX\n",
      "Prediction for sentence 98: AIMX\n",
      "Prediction for sentence 99: AIMX\n",
      "Prediction for sentence 100: AIMX\n",
      "Prediction for sentence 101: AIMX\n",
      "Prediction for sentence 102: AIMX\n",
      "Prediction for sentence 103: AIMX\n",
      "Prediction for sentence 104: AIMX\n",
      "Prediction for sentence 105: AIMX\n",
      "Prediction for sentence 106: AIMX\n",
      "Prediction for sentence 107: AIMX\n",
      "Prediction for sentence 108: AIMX\n",
      "Prediction for sentence 109: AIMX\n",
      "Prediction for sentence 110: AIMX\n",
      "Prediction for sentence 111: AIMX\n",
      "Prediction for sentence 112: AIMX\n",
      "Prediction for sentence 113: AIMX\n",
      "Prediction for sentence 114: AIMX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\annaj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Labels for the categories\n",
    "categories = ['AIMX', 'OWNX', 'CONT', 'BASE', 'NUMBER', 'MISC']\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB(class_prior=None, fit_prior=True)\n",
    "\n",
    "# Assuming each line in your labeled dataset corresponds to a label\n",
    "labels = []\n",
    "for file in os.listdir(label):\n",
    "    if os.path.isfile(os.path.join(label, file)):\n",
    "        with open(os.path.join(label, file), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                labels.append(line.strip())\n",
    "\n",
    "# Convert labels to numerical values using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_label, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Now, use the trained classifier to predict labels for the unlabeled dataset\n",
    "unlabeled_predictions = classifier.predict(tfidf_unlabel)\n",
    "\n",
    "# Print the predicted labels for the unlabeled dataset\n",
    "for i, prediction in enumerate(unlabeled_predictions):\n",
    "    print(f\"Prediction for sentence {i + 1}: {categories[prediction]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383fa01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d52ebb92",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Task 4: Summarize your work and your findings in a few sentences.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2fc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f139d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8ecbb8",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue;\">BONUS [Extra 5% added to any workshops]: Use other classification approaches to label the unlabeled sentences. Evaluate your work using precision, recall, and f1 score.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e055a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2187d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfeaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a147eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd6548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
